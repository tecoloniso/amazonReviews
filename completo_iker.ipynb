{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64703665-d654-4d7d-957c-b55bd551b20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alumno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alumno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alumno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alumno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Descargar recursos de NLTK (si es la primera vez)\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efae2505-0e69-4002-b562-9734c61ef4cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reviews_cascos_gaming_desbalanceado.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar los datos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreviews_cascos_gaming_desbalanceado.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py311ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py311ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\py311ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py311ml\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\.conda\\envs\\py311ml\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews_cascos_gaming_desbalanceado.csv'"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv(\"reviews_cascos_gaming_desbalanceado.csv\",sep=';', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50b631-ac59-4a25-8997-aaf1ab2a0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('spanish')\n",
    "\n",
    "def quitar_tildes(texto):\n",
    "    texto = unicodedata.normalize(\"NFD\", texto)  # Descomponer caracteres con tilde\n",
    "    texto = \"\".join(c for c in texto if unicodedata.category(c) != \"Mn\")  # Eliminar solo las tildes\n",
    "    return unicodedata.normalize(\"NFC\", texto)  # Recomponer texto normalizado\n",
    "    \n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r\"[^a-zA-Z\\s]\", \" \", texto)\n",
    "    texto = quitar_tildes(texto)\n",
    "    texto = texto.lower()\n",
    "    palabras = word_tokenize(texto)\n",
    "    palabras = [lemmatizer.lemmatize(palabra) for palabra in palabras] \n",
    "    # Stemming\n",
    "    # palabras = [porter.stem(palabra) for palabra in word_tokenize(texto)]\n",
    "    # palabras = [snowball.stem(palabra) for palabra in word_tokenize(texto)]\n",
    "    # Eliminar stopwords\n",
    "    palabras = [palabra for palabra in palabras if palabra not in stopwords.words(\"spanish\")]\n",
    "    return \" \".join(palabras)\n",
    "\n",
    "def sin_limpiar_texto(texto):\n",
    "    return texto\n",
    "\n",
    "data[\"review\"] = data[\"review\"].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import unicodedata\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Lista de palabras ambiguas que deben ser consideradas en bigramas\n",
    "# palabras_ambiguas = {\"calidad\", \"funciona\", \"buena\", \"malo\", \"producto\", \"sonido\", \"precio\", \"recomiendo\"}\n",
    "\n",
    "# def procesar_texto_con_bigramas(texto):\n",
    "#     # Quitar caracteres especiales y convertir a min칰sculas\n",
    "#     texto = re.sub(r\"[^a-zA-Z\\s]\", \" \", texto)\n",
    "#     texto = quitar_tildes(texto).lower()\n",
    "    \n",
    "#     # Tokenizar palabras\n",
    "#     palabras = word_tokenize(texto)\n",
    "    \n",
    "#     # Construir bigramas manualmente\n",
    "#     bigramas = [\"_\".join(pair) for pair in zip(palabras, palabras[1:])]\n",
    "    \n",
    "#     # Filtrar bigramas que contienen al menos una palabra ambigua\n",
    "#     palabras_finales = {big for big in bigramas if any(word in big for word in palabras_ambiguas)}\n",
    "    \n",
    "#     # Si hay menos de 3 bigramas, agregar palabras ambiguas individuales para asegurar contenido\n",
    "#     if len(palabras_finales) < 3:\n",
    "#         palabras_finales.update(set(palabra for palabra in palabras if palabra in palabras_ambiguas))\n",
    "    \n",
    "#     return \" \".join(palabras_finales)\n",
    "\n",
    "# # Aplicar preprocesamiento\n",
    "# data[\"review\"] = data[\"review\"].apply(procesar_texto_con_bigramas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"reviews_cascos_gaming.csv\",sep=';', encoding=\"utf-8\")\n",
    "\n",
    "# top = 15\n",
    "\n",
    "# # Cargar datos\n",
    "# rese침as = data['review'].astype(str).fillna(\"\").apply(limpiar_texto)\n",
    "\n",
    "# # Vectorizaci칩n con BoW\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = rese침as\n",
    "# y = data['stars']\n",
    "\n",
    "# # Dividir en entrenamiento, validaci칩n y prueba\n",
    "# X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.5, random_state=12)\n",
    "\n",
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # Entrenar el modelo de Regresi칩n Log칤stica\n",
    "# modelo_lr = LogisticRegression(random_state=12, class_weight=\"balanced\", C=1, max_iter=1000)\n",
    "# modelo_lr.fit(X_train, Y_train)\n",
    "\n",
    "# # Obtener coeficientes de importancia\n",
    "# coeficientes = np.abs(modelo_lr.coef_).mean(axis=0)  # Promedio de coeficientes en todas las clases\n",
    "\n",
    "# # Obtener vocabulario\n",
    "# vocabulario = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# # Seleccionar las Top N palabras con m치s peso en el modelo\n",
    "# top_indices = np.argsort(coeficientes)[-top:]  # 칈ndices de las palabras m치s influyentes\n",
    "# palabras_clave = set(vocabulario[top_indices])  # Convertir a conjunto para b칰squeda r치pida\n",
    "\n",
    "# print(f\"Palabras clave seleccionadas: {palabras_clave}\")\n",
    "\n",
    "\n",
    "# def filtrar_rese침a(rese침a, palabras_clave):\n",
    "#     palabras = rese침a.split()  # Separar en palabras\n",
    "#     palabras_filtradas = [palabra for palabra in palabras if palabra in palabras_clave]\n",
    "#     return \" \".join(palabras_filtradas) if palabras_filtradas else rese침a  # Si no hay palabras clave, conservar el original\n",
    "\n",
    "# # Aplicar la funci칩n a las rese침as de validaci칩n\n",
    "# # rese침as_val = data.loc[X_val.indices, 'review'].astype(str).fillna(\"\")\n",
    "# X_val = X_val.apply(lambda x: filtrar_rese침a(x, palabras_clave))\n",
    "\n",
    "# # data.loc[X_val.indices, 'review_filtrada'] = rese침as_filtradas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fc065-6925-426e-a885-79b36a95eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val.to_csv(\"review_preprocesada.csv\", index=False, sep=';')\n",
    "# print(X_val)\n",
    "# print(len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd1678-af1e-41d4-8846-e5513658eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rese침as = rese침as_filtradas.astype(str).fillna(\"\")\n",
    "rese침as = data['review'].astype(str).fillna(\"\")\n",
    "\n",
    "# Vectorizaci칩n BoW\n",
    "def bow(rese침as):\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(rese침as)\n",
    "\n",
    "# Vectorizacion TIF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),max_features=5000)\n",
    "X = vectorizer.fit_transform(rese침as)\n",
    "\n",
    "# X = bow(rese침as)\n",
    "y = data['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = vectorizer.transform(X_val)\n",
    "# X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Vectorizaci칩n con Bag of Words\n",
    "# vectorizer = CountVectorizer(max_features=5000)\n",
    "# X = vectorizer.fit_transform(rese침as)\n",
    "\n",
    "# # Separar datos en entrenamiento y prueba\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# # Entrenar modelo Naive Bayes\n",
    "# modelo_nb = MultinomialNB(alpha=2)\n",
    "# modelo_nb.fit(X_train, Y_train)\n",
    "\n",
    "# # Predecir y evaluar\n",
    "# y_test_pred = modelo_nb.predict(X_test)\n",
    "# acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "# print(f\"Accuracy de Naive Bayes: {acc}%\")\n",
    "\n",
    "# # Obtener palabras m치s relevantes\n",
    "# vocabulario = vectorizer.get_feature_names_out()\n",
    "# probabilidades = np.exp(modelo_nb.feature_log_prob_)\n",
    "\n",
    "# # # Crear histogramas por cada estrella\n",
    "# # for i, estrellas in enumerate(modelo_nb.classes_):\n",
    "# #     # Seleccionar las 15 palabras m치s relevantes\n",
    "# #     top_indices = np.argsort(probabilidades[i])[-15:]  # Tomamos las 15 m치s altas\n",
    "# #     palabras_top = [vocabulario[idx] for idx in top_indices]\n",
    "# #     probabilidades_top = [probabilidades[i, idx] for idx in top_indices]\n",
    "\n",
    "# #     # Graficar histograma\n",
    "# #     plt.figure(figsize=(10, 5))\n",
    "# #     plt.barh(palabras_top, probabilidades_top, color='skyblue')\n",
    "# #     plt.xlabel(\"Probabilidad\")\n",
    "# #     plt.ylabel(\"Palabras\")\n",
    "# #     plt.title(f\" Palabras m치s influyentes para {estrellas} estrellas\")\n",
    "# #     plt.gca().invert_yaxis()  # Invertir eje Y para que la palabra m치s relevante est칠 arriba\n",
    "# #     plt.show()\n",
    "\n",
    "\n",
    "# # Crear figura con 2 filas y 2 columnas\n",
    "# fig, axes = plt.subplots(3, 2, figsize=(12, 10))  \n",
    "# axes = axes.flatten()  # Aplanar la matriz de ejes para iterar f치cilmente\n",
    "\n",
    "# for i, estrellas in enumerate(modelo_nb.classes_):\n",
    "#     # Seleccionar las 15 palabras m치s relevantes\n",
    "#     top_indices = np.argsort(probabilidades[i])[-15:]  # Tomamos las 15 m치s altas\n",
    "#     palabras_top = [vocabulario[idx] for idx in top_indices]\n",
    "#     probabilidades_top = [probabilidades[i, idx] for idx in top_indices]\n",
    "\n",
    "#     # Graficar en la subfigura correspondiente\n",
    "#     ax = axes[i]\n",
    "#     ax.barh(palabras_top, probabilidades_top, color='skyblue')\n",
    "#     ax.set_xlabel(\"Probabilidad\")\n",
    "#     ax.set_ylabel(\"Palabras\")\n",
    "#     ax.set_title(f\"Palabras m치s influyentes para {estrellas}救\")\n",
    "#     ax.invert_yaxis()  # Invertir eje Y para que la palabra m치s relevante est칠 arriba\n",
    "\n",
    "# # Ajustar el dise침o para evitar superposiciones\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00550e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Simulaci칩n de etiquetas reales y predichas \n",
    "y_true = np.array([5, 4, 3, 5, 2, 1, 4, 3, 5, 2])\n",
    "y_pred = np.array([5, 3, 3, 4, 2, 1, 4, 2, 5, 3])\n",
    "\n",
    "# Crear matriz de confusi칩n\n",
    "def matriz_confusion(y,y_pred):   \n",
    "    cm = confusion_matrix(y, y_pred, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "    # Visualizar la matriz de confusi칩n\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "    plt.xlabel(\"Clase Predicha\")\n",
    "    plt.ylabel(\"Clase Real\")\n",
    "    plt.title(\"Matriz de Confusi칩n\")\n",
    "    plt.show()\n",
    "    return\n",
    "# matriz_confusion(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribucion_estrellas(y):\n",
    "    # Crear histograma de la distribuci칩n de estrellas\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(y, bins=range(1, 8), edgecolor='black', align='left')\n",
    "    plt.xticks(range(1, 6))  # Ajustar ticks a las puntuaciones de 1 a 5\n",
    "    plt.xlabel(\"Estrellas\")\n",
    "    plt.ylabel(\"N칰mero de rese침as\")\n",
    "    plt.title(\"Distribuci칩n de rese침as por puntuaci칩n de estrellas\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "distribucion_estrellas(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_predicciones(y, y_pred):\n",
    "    y = np.array(y)  \n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Crear 칤ndices con espaciado uniforme m치s grande\n",
    "    x_indices = np.linspace(0, len(y) * 2.5, len(y))  # Factor 2.5 para mayor separaci칩n\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Graficar la clase real vs la predicha con separaci칩n uniforme\n",
    "    plt.scatter(x_indices, y, color=\"blue\", label=\"Clase Real\", marker=\"o\", s=100)\n",
    "    plt.scatter(x_indices, y_pred, color=\"red\", label=\"Clase Predicha\", marker=\"x\", s=100)\n",
    "\n",
    "    # Conectar con l칤neas para ver las diferencias\n",
    "    for i in range(len(y)):\n",
    "        plt.plot([x_indices[i], x_indices[i]], [y[i], y_pred[i]], color=\"gray\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.xlabel(\"칈ndice de Muestra (Separaci칩n Uniforme)\")\n",
    "    plt.ylabel(\"Clase\")\n",
    "    plt.title(\"Comparaci칩n entre Clases Reales y Predichas\")\n",
    "    plt.xticks([])  # Ocultar etiquetas en el eje X para evitar saturaci칩n visual\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# distancia_predicciones(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeff821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def distancia_predicciones_barras(y, y_pred):\n",
    "    y = np.array(y)  \n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calcular la diferencia entre la clase real y la predicha\n",
    "    diferencia = y - y_pred  \n",
    "\n",
    "    # Crear etiquetas para cada muestra\n",
    "    indices = np.arange(len(y))\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Barras para clase real\n",
    "    plt.bar(indices, y, color='blue', alpha=0.6, label='Clase Real')\n",
    "\n",
    "    # Barras para diferencia (positiva o negativa)\n",
    "    plt.bar(indices, diferencia, color='red', alpha=0.6, label='Diferencia (Real - Predicha)', bottom=y)\n",
    "\n",
    "    # Agregar valores en las barras\n",
    "    for i in range(len(y)):\n",
    "        plt.text(indices[i], y[i] + diferencia[i] / 2, f\"{y_pred[i]}\", ha='center', va='center', \n",
    "                 color=\"white\", fontsize=12, fontweight=\"bold\", bbox=dict(facecolor='black', alpha=0.5, edgecolor='none', pad=2))\n",
    "\n",
    "    # Expandir los l칤mites del eje Y para evitar cortes\n",
    "    plt.ylim(0, max(y) + 2)\n",
    "\n",
    "    plt.xlabel(\"칈ndice de Muestra\")\n",
    "    plt.ylabel(\"Clase\")\n",
    "    plt.title(\"Diferencia entre Clases Reales y Predichas\")\n",
    "    plt.xticks(indices)  # Mostrar cada 칤ndice\n",
    "    plt.yticks(range(1, max(y) + 2))  # Ajustar para las clases\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso con datos de prueba\n",
    "y_true = np.array([5, 4, 3, 5, 2, 1, 4, 3, 5, 2])\n",
    "y_pred = np.array([5, 3, 3, 4, 2, 1, 4, 2, 5, 3])\n",
    "\n",
    "# distancia_predicciones_barras(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47036a6d-976f-47b5-aa99-c6dc8f57f5fc",
   "metadata": {},
   "source": [
    "# **ENTRENAMIENTO DEL MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f3498-5931-4136-baf3-830d65d652ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "# X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Opci칩n 1: Submuestreo (reduce la cantidad de datos de la clase mayoritaria)\n",
    "# undersampler = RandomUnderSampler(random_state=12)\n",
    "# X_train, Y_train = undersampler.fit_resample(X_train, Y_train)\n",
    "# print(\"Datos balanceados con submuestreo:\")\n",
    "# print(Y_train.value_counts())\n",
    "\n",
    "# Opci칩n 2: Sobremuestreo (aumenta la cantidad de datos de la clase minoritaria)\n",
    "# oversampler = RandomOverSampler(random_state=12)\n",
    "# X_train, Y_train = oversampler.fit_resample(X_train, Y_train)\n",
    "# print(\"Datos balanceados con sobremuestreo:\")\n",
    "# print(Y_train.value_counts())\n",
    "\n",
    "\n",
    "distribucion_estrellas(y)\n",
    "distribucion_estrellas(Y_train)\n",
    "distribucion_estrellas(Y_test)\n",
    "\n",
    "def entrenar_modelo(X_train, X_test, Y_train, Y_test):\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=12)\n",
    "    modelos = {\n",
    "        \"Naive-Bayes\": MultinomialNB(),\n",
    "        \"LogisticRegression\": LogisticRegression(random_state=12, class_weight=\"balanced\"),\n",
    "        \"RedNeuronal\": MLPClassifier(solver='adam', max_iter=300, random_state=12)\n",
    "    }\n",
    "    \n",
    "    # Hiperpar치metros para GridSearch\n",
    "    hiperparametros = {\n",
    "        \"Naive-Bayes\": {\"alpha\": [0.1, 0.5, 1, 2, 5]},\n",
    "        \"LogisticRegression\": {\n",
    "            \"C\": [0.1, 1, 10, 100],\n",
    "            \"penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "            \"solver\": [\"lbfgs\", \"liblinear\", \"saga\"],\n",
    "            \"max_iter\": [100, 200, 500]\n",
    "        },\n",
    "        \"RedNeuronal\": {\n",
    "            \"hidden_layer_sizes\": [(50,), (100,)],\n",
    "            # \"alpha\": [ 0.001, 0.01],\n",
    "            # \"activation\": [\"relu\", \"tanh\"],\n",
    "            \"solver\": [\"adam\", \"lbfgs\"]\n",
    "            # \"batch_size\": [16, 32]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    mejor_modelo = None\n",
    "    mejor_acc = 0\n",
    "    \n",
    "    for nombre, modelo in modelos.items():\n",
    "        grid = GridSearchCV(modelo, hiperparametros[nombre], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X_train, Y_train)\n",
    "        \n",
    "        # Evaluar en conjunto de validaci칩n\n",
    "        y_val_pred = grid.best_estimator_.predict(X_val)\n",
    "        acc_val = round(accuracy_score(Y_val, y_val_pred) * 100, 2)\n",
    "        \n",
    "        print(f\"Mejor modelo para {nombre}: {grid.best_params_}\")\n",
    "        print(f\"Accuracy en validaci칩n: {acc_val}%\")\n",
    "        print(classification_report(Y_val, y_val_pred))\n",
    "        matriz_confusion(Y_val,y_val_pred)\n",
    "        # matriz_confusion(Y_test,y_pred)\n",
    "        # mascara = Y_test==5\n",
    "        # y_test_5 = Y_test[mascara]\n",
    "        # y_pred_5 = y_pred[mascara]\n",
    "        # distancia_predicciones(y_test_5,y_pred_5)\n",
    "        # Evaluar en conjunto de prueba solo si es el mejor modelo\n",
    "        if acc_val > mejor_acc:\n",
    "            mejor_acc = acc_val\n",
    "            mejor_modelo = grid.best_estimator_\n",
    "        \n",
    "    # Evaluaci칩n final en test\n",
    "    y_test_pred = mejor_modelo.predict(X_test)\n",
    "    acc_test = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "    print(f\"Mejor modelo final: {mejor_modelo}\")\n",
    "    print(f\"Accuracy en test: {acc_test}%\")\n",
    "    print(classification_report(Y_test, y_test_pred))\n",
    "    matriz_confusion(Y_test,y_test_pred)\n",
    "    \n",
    "    return mejor_modelo, acc_test\n",
    "\n",
    "# Llamada a la funci칩n\n",
    "mejor_modelo, mejor_acc = entrenar_modelo(X_train, X_test, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3bb97-267f-401b-ae80-e7a34cd73a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear un DataFrame con ejemplos conflictivos\n",
    "data2 = {\n",
    "    \"review\": [\n",
    "        \"me gusta mucho,muy bien\",  # Negativo, pero podr칤a ser subjetivo\n",
    "        \"no funciona,mal\",  # Claramente negativo\n",
    "        \"pens칠 que ser칤a mejor\",  # Ambiguo, sugiere decepci칩n pero no es expl칤cito\n",
    "        \"es diferente a lo que esperaba\",  # No es claramente positivo ni negativo\n",
    "        \"me sorprendi칩\",  # Puede ser positivo o negativo seg칰n el contexto\n",
    "    ],\n",
    "    \"stars\": [5, 1, 2, 3, 4]  # Asignaci칩n de estrellas con ambig칲edad\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(data2)\n",
    "print(df2)\n",
    "df2[\"review\"] = df2[\"review\"].apply(limpiar_texto)\n",
    "rese침as = df2['review'].astype(str).fillna(\"\")\n",
    "print(rese침as)\n",
    "Xteest = vectorizer.transform(rese침as)\n",
    "print(Xteest.shape)\n",
    "yteest = df2['stars']\n",
    "print(yteest.shape)\n",
    "acc = entrenar_modelo(X_train, Xteest, Y_train, yteest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee694c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Entrenar modelo de Regresi칩n Log칤stica\n",
    "# modelo_lr = LogisticRegression(random_state=12, class_weight=\"balanced\", C=1, max_iter=1000)\n",
    "# modelo_lr.fit(X_train, Y_train)\n",
    "\n",
    "# # Obtener coeficientes de importancia\n",
    "# coeficientes = modelo_lr.coef_  # (n_clases, n_palabras)\n",
    "\n",
    "# # Obtener vocabulario\n",
    "# vocabulario = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Mostrar las palabras m치s influyentes por clase\n",
    "# for i, estrellas in enumerate(modelo_lr.classes_):\n",
    "#     top_indices = np.argsort(coeficientes[i])[-15:]  # Tomamos las 15 palabras m치s importantes\n",
    "#     palabras_top = [vocabulario[idx] for idx in top_indices]\n",
    "#     coef_top = [coeficientes[i, idx] for idx in top_indices]\n",
    "\n",
    "#     print(f\"\\n游댳 Palabras m치s influyentes para {estrellas} estrellas:\")\n",
    "#     for palabra, coef in zip(palabras_top, coef_top):\n",
    "#         print(f\"{palabra} (score: {coef:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f785330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Entrenar modelo de Regresi칩n Log칤stica\n",
    "# modelo_lr = LogisticRegression(random_state=12, class_weight=\"balanced\", C=1, max_iter=1000)\n",
    "# modelo_lr.fit(X_train, Y_train)\n",
    "\n",
    "# # Obtener coeficientes de importancia\n",
    "# coeficientes = modelo_lr.coef_  # (n_clases, n_palabras)\n",
    "\n",
    "# # Obtener vocabulario\n",
    "# vocabulario = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Inicializar una lista para guardar los porcentajes\n",
    "# porcentajes_por_clase = []\n",
    "\n",
    "# # Calcular los porcentajes de cada palabra para cada clase\n",
    "# for i, estrellas in enumerate(modelo_lr.classes_):\n",
    "#     coef_class = coeficientes[i]\n",
    "#     total_coef = np.sum(np.abs(coef_class))  # Sumamos los coeficientes absolutos de la clase\n",
    "#     porcentajes = (np.abs(coef_class) / total_coef) * 100  # Convertimos en porcentaje\n",
    "    \n",
    "#     # Guardamos los porcentajes junto con las palabras\n",
    "#     top_indices = np.argsort(coef_class)[-15:]  # Tomamos las 15 palabras m치s importantes\n",
    "#     palabras_top = [vocabulario[idx] for idx in top_indices]\n",
    "#     porcentajes_top = [porcentajes[idx] for idx in top_indices]\n",
    "    \n",
    "#     porcentajes_por_clase.append((palabras_top, porcentajes_top))\n",
    "\n",
    "# # Crear una cuadr칤cula de subgr치ficas (3 filas, 2 columnas)\n",
    "# fig, axes = plt.subplots(3, 2, figsize=(12, 10))  # Tama침o reducido\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Crear un gr치fico por clase de estrellas\n",
    "# for i, (palabras_top, porcentajes_top) in enumerate(porcentajes_por_clase):\n",
    "#     ax = axes[i]  # Acceder a cada subgr치fico\n",
    "#     ax.barh(palabras_top, porcentajes_top, color='skyblue')\n",
    "#     ax.set_title(f'Porcentajes de palabras para {modelo_lr.classes_[i]} estrellas')\n",
    "#     ax.set_xlabel('Porcentaje')\n",
    "#     ax.set_ylabel('Palabras')\n",
    "\n",
    "# # Ajustar el dise침o para evitar solapamientos\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb55726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_acc = []\n",
    "\n",
    "# def limpiar_texto(texto):\n",
    "#     return texto.lower()\n",
    "\n",
    "# for i in range(10,20):\n",
    "\n",
    "#     data = pd.read_csv(\"reviews_cascos_gaming.csv\",sep=';', encoding=\"utf-8\")\n",
    "\n",
    "#     top = i\n",
    "\n",
    "#     # Cargar datos\n",
    "#     rese침as = data['review'].astype(str).fillna(\"\").apply(limpiar_texto)\n",
    "\n",
    "#     # Vectorizaci칩n con BoW\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     X = rese침as\n",
    "#     y = data['stars']\n",
    "\n",
    "#     # Dividir en entrenamiento, validaci칩n y prueba\n",
    "#     X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "#     X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.5, random_state=12)\n",
    "\n",
    "#     X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "#     # Entrenar el modelo de Regresi칩n Log칤stica\n",
    "#     modelo_lr = LogisticRegression(random_state=12, class_weight=\"balanced\", C=1, max_iter=1000)\n",
    "#     modelo_lr.fit(X_train, Y_train)\n",
    "\n",
    "#     # Obtener coeficientes de importancia\n",
    "#     coeficientes = np.abs(modelo_lr.coef_).mean(axis=0)  # Promedio de coeficientes en todas las clases\n",
    "\n",
    "#     # Obtener vocabulario\n",
    "#     vocabulario = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "#     # Seleccionar las Top N palabras con m치s peso en el modelo\n",
    "#     top_indices = np.argsort(coeficientes)[-top:]  # 칈ndices de las palabras m치s influyentes\n",
    "#     palabras_clave = set(vocabulario[top_indices])  # Convertir a conjunto para b칰squeda r치pida\n",
    "\n",
    "\n",
    "#     # Aplicar la funci칩n a las rese침as de validaci칩n\n",
    "#     # rese침as_val = data.loc[X_val.indices, 'review'].astype(str).fillna(\"\")\n",
    "#     X_val = X_val.apply(lambda x: filtrar_rese침a(x, palabras_clave))\n",
    "\n",
    "#     # data.loc[X_val.indices, 'review_filtrada'] = rese침as_filtradas\n",
    "\n",
    "\n",
    "#     X_val = vectorizer.transform(X_val)\n",
    "#     X_test = vectorizer.transform(X_test)\n",
    "\n",
    "#     lista_acc.append(entrenar_modelo(X_val,X_test,Y_val,Y_test))\n",
    "\n",
    "# # Graficar los resultados\n",
    "# plt.plot(range(10, 20), lista_acc, marker='o')\n",
    "# plt.xlabel('Top N palabras')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.title('Impacto del n칰mero de palabras clave en el modelo')\n",
    "# plt.xticks(range(10, 20))\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def limpiar_texto(texto):\n",
    "#     return texto#.lower()\n",
    "\n",
    "# data = pd.read_csv(\"reviews_cascos_gaming.csv\",sep=';', encoding=\"utf-8\")\n",
    "\n",
    "# # Cargar datos\n",
    "# rese침as = data['review'].astype(str).fillna(\"\").apply(limpiar_texto)\n",
    "\n",
    "# # Vectorizaci칩n con BoW\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = rese침as\n",
    "# y = data['stars']\n",
    "\n",
    "# # Dividir en entrenamiento, validaci칩n y prueba\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# X_train = vectorizer.fit_transform(X_train)\n",
    "# X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# entrenar_modelo(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20509e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py311ml]",
   "language": "python",
   "name": "conda-env-.conda-py311ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
